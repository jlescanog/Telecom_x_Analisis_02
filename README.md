# Telecom X - Parte 2: Modelado Predictivo de Churn de Clientes

Este repositorio contiene la segunda fase del proyecto de an谩lisis de datos para Telecom X, centrada en la construcci贸n y evaluaci贸n de modelos de Machine Learning para predecir la evasi贸n de clientes (Churn).

##  Prop贸sito del An谩lisis

El objetivo principal de este proyecto es desarrollar un modelo predictivo robusto capaz de identificar a los clientes con alta probabilidad de cancelar su servicio. Al predecir el churn de manera proactiva, Telecom X puede implementar estrategias de retenci贸n dirigidas, reducir la p茅rdida de ingresos y mejorar la lealtad de sus clientes.

##  Estructura del Proyecto

El repositorio est谩 organizado de la siguiente manera para facilitar su comprensi贸n y ejecuci贸n:
## 锔 Proceso de Preparaci贸n de Datos

Para asegurar el m谩ximo rendimiento de los modelos, los datos (provenientes del archivo datos_para_modelo.csv) pasaron por un riguroso proceso de preprocesamiento:

1.  **Clasificaci贸n y Codificaci贸n de Variables:**
    *   Las variables categ贸ricas (ej. Contract, InternetService) fueron transformadas a un formato num茅rico utilizando la t茅cnica de **One-Hot Encoding** (pd.get_dummies). Esto es esencial, ya que los algoritmos de Machine Learning operan con datos num茅ricos.

2.  **Manejo del Desbalance de Clases:**
    *   Se detect贸 un desbalance moderado en la variable objetivo Churn (74% de clientes activos vs. 26% que cancelaron). Para corregirlo, se aplic贸 la t茅cnica **SMOTE (Synthetic Minority Over-sampling Technique)**, que gener贸 ejemplos sint茅ticos de la clase minoritaria, resultando en un conjunto de datos perfectamente balanceado (50/50).

3.  **Estandarizaci贸n de Datos:**
    *   Todas las caracter铆sticas num茅ricas fueron estandarizadas utilizando StandardScaler de Scikit-learn. Este paso asegura que todas las variables tengan una media de 0 y una desviaci贸n est谩ndar de 1, evitando que modelos sensibles a la escala (como la Regresi贸n Log铆stica) se vean sesgados por la magnitud de los datos.

4.  **Separaci贸n en Entrenamiento y Prueba:**
    *   El conjunto de datos final fue dividido en un **80% para entrenamiento** y un **20% para prueba** (train_test_split). Esta separaci贸n es crucial para evaluar de manera objetiva el rendimiento de los modelos en datos que nunca han visto.

##  Justificaci贸n de la Modelizaci贸n

Se entrenaron dos modelos con enfoques diferentes para tener una visi贸n comparativa:

*   **Regresi贸n Log铆stica:** Se eligi贸 como un modelo de base (baseline) por su simplicidad, rapidez y alta interpretabilidad a trav茅s de sus coeficientes. Requiere que los datos est茅n estandarizados para funcionar correctamente.
*   **Random Forest:** Se seleccion贸 como un modelo m谩s complejo y potente, basado en 谩rboles de decisi贸n. No es sensible a la escala de los datos, pero se beneficia de un buen preprocesamiento. Es conocido por su alto rendimiento y su capacidad para medir la importancia de las variables.

El modelo **Random Forest** demostr贸 un desempe帽o superior, con una **exactitud del 85%**, y sus resultados sobre la importancia de las variables fueron m谩s consistentes con el an谩lisis exploratorio.

##  Ejemplo de Insight del An谩lisis (EDA)

Durante el an谩lisis, se confirm贸 que la **antig眉edad (tenure)** y el **tipo de contrato** son los factores m谩s determinantes. El siguiente gr谩fico ilustra c贸mo los clientes con contratos mes a mes son significativamente m谩s propensos a la cancelaci贸n.

##  Instrucciones para Ejecutar el Cuaderno

Para replicar este an谩lisis, sigue los siguientes pasos en Google Colab:

1.  **Cargar los Datos:**
    *   Sube el archivo datos_para_modelo.csv a tu sesi贸n de Google Colab o m贸ntalo desde tu Google Drive.
    *   Aseg煤rate de ajustar la ruta del archivo en la celda de carga de datos del notebook: `pd.read_csv("tu/ruta/a/datos_para_modelo.csv")`.

2.  **Instalar Bibliotecas:**
    *   El notebook requiere la biblioteca imbalanced-learn para el balanceo de datos. La primera celda de c贸digo del notebook incluye el comando para instalarla. Aseg煤rate de ejecutarla:
        `!pip install imbalanced-learn`

3.  **Ejecutar el Notebook:**
    *   Una vez cargados los datos y la biblioteca instalada, puedes ejecutar todas las celdas en orden desde el men煤 Entorno de ejecuci贸n > Ejecutar todo. El cuaderno te guiar谩 a trav茅s de cada paso del preprocesamiento, modelado y evaluaci贸n.
